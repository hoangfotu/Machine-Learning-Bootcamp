{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "829f479a",
   "metadata": {},
   "source": [
    "<img align=\"center\" src=\"https://iili.io/3wI8gI.png\" style=\"height:90px\" style=\"width:30px\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6032deb8",
   "metadata": {},
   "source": [
    "# Multiple Linear Regression â€“ Predicting Car Prices\n",
    "\n",
    "In this lab your task is to build a Machine Learning algorithm that predicts the sales price of a car.\n",
    "\n",
    "Picking the right data is a pivotal skill in machine learning, often overshadowing advanced algorithms. They enhance model performance by ensuring relevance and quality of data, thereby increasing accuracy and reducing complexity. To create the best possible model in this lab, you'll need to take time to think and reason about what drives the prices of cars.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a01150e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# run cell to import libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2bf4a9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# run this cell to see explaination of columns\n",
    "pd.read_excel(\"Data Dictionary - carprices.xlsx\", header=3, usecols=[\"Unnamed: 7\", \"Unnamed: 11\"], nrows=26)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "313ae200",
   "metadata": {},
   "source": [
    "## Import the `CarPrice_Assignment.csv` file and inspect the data carefully"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a74b3556",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0514b7f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad2ae0c5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "cf80410f",
   "metadata": {},
   "source": [
    "## Transforming text to numbers\n",
    "\n",
    "Supervised machine learning models can only understand numerical data which means that whenever we have text data in our dataset, we need to figure out a way to transform it into numbers. If you run the code below, you can see the different unique values in the object columns. There are different strategies to transform text into numbers.\n",
    "\n",
    "Look at the output of each column and ask yourself: is column categorical and likely nominal, meaning they have no inherent order? Or, does this column seem to represent ordinal data, does it indicate a natural ordering? Or, does this column have too many unique values?\n",
    "\n",
    "```python\n",
    "for i in df.select_dtypes(\"object\").columns:\n",
    "    print(i)\n",
    "    print(df[i].unique())\n",
    "    print()\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d1c3301",
   "metadata": {},
   "outputs": [],
   "source": [
    "# look at the number of unique values in the columns that have object data type"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9657f76",
   "metadata": {},
   "source": [
    "## Pick a transformation strategy\n",
    "\n",
    "Depending on how you classified the columns in the previous question, you now have three options to choose from. If the columns is categorical and likely nominal, you should turn the data into dummies. If it represent ordinal data, you should map out numerical values for each text value. And finally, if the column has too many unique values, you should drop it.\n",
    "\n",
    "<strong>To turn the columns into dummies:</strong>\n",
    "```python\n",
    "df = pd.get_dummies(data=df, columns=[\"here you put the columns you want to dummy\"], drop_first=True)\n",
    "```\n",
    "\n",
    "<strong>To map out new numerical values for each text value:</strong>\n",
    "```python\n",
    "df[\"doornumber\"] = df[\"doornumber\"].map({\"two\":2, \"four\":4})\n",
    "```\n",
    "\n",
    "<strong>If the column has too many unique values:</strong>\n",
    "```python\n",
    "df.drop(\"column_you_want_to_drop\", axis=1, inplace=True)\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acdd8f1d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a967f615",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94169e7c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7722c75b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d983f366",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "75c0f549",
   "metadata": {},
   "source": [
    "## Exploratory Data Analysis\n",
    "\n",
    "Use the `df.corr(numeric_only=True)` and then select the `price` column to see how your dependent variable correlates with your independent variables. You can also use the `sort_values` method and pass `key=abs` to sort the columns by correlation in absolute numbers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1446df2e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "80f88f76",
   "metadata": {},
   "source": [
    "### Pick the nine most correlating independent variables and visualize them as regression plots\n",
    "\n",
    "Use the `sns.regplot()` and pass the independent variable to the `x` axis and set `y=\"price\"`. Remember to set `data=df`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6ead0b0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "d493b14f",
   "metadata": {},
   "source": [
    "### Pick the nine most correlating independent variables and visualize them as histogram\n",
    "\n",
    "Use the `sns.histplot()` and pass the independent variable to the `x` axis. Remember to set `data=df`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd5aabb6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "9c052c46",
   "metadata": {},
   "source": [
    "## Independent variables and dependent variable split\n",
    "\n",
    "Set your independent variables equal to `X` and you dependent variable to `y`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84d984a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the data into X and y\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef5f8a96",
   "metadata": {},
   "source": [
    "## Scaling the data\n",
    "\n",
    "Scaling data in machine learning, particularly for algorithms like Linear Regression, is crucial as it ensures uniformity across features. Many algorithms, including Linear Regression, assume that all features have similar scales and are centered around zero. This uniformity is essential because it allows each feature to contribute equally to the model's predictions, preventing features with larger scales from disproportionately influencing the model. Additionally, for algorithms that rely on gradient descent for optimization, scaling can significantly improve the speed and stability of convergence towards the minimum.\n",
    "\n",
    "However, when it comes to dummy variables, scaling is generally not performed. This is because dummy variables are binary, representing the presence or absence of a category with values 0 or 1, and scaling them would lead to a loss of this clear binary interpretation. Moreover, since dummy variables are already on a consistent scale, additional scaling does not provide the same benefits as it does for continuous variables. In the context of Linear Regression, keeping dummy variables unscaled maintains interpretability and efficiency without adversely affecting the model's performance.\n",
    "\n",
    "\n",
    "<strong>Execute the follwing list comprehensions to separate the binary data from the data that we are going to scale. </strong>\n",
    "    \n",
    "```python\n",
    "non_binary = X[[col for col in X.columns if X[col].nunique() > 2]]\n",
    "binary = X[[col for col in X.columns if X[col].nunique() <= 2]]\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "565f98dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# execute code here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8db78c0",
   "metadata": {},
   "source": [
    "## Calculate the z-score\n",
    "\n",
    "\n",
    "Calculating the z-score in the context of linear regression is primarily done to standardize the features, making them more comparable and interpretable. The z-score, a measure of how many standard deviations a data point is from the mean, transforms the features so that they have a mean of zero and a standard deviation of one. This standardization is particularly useful in linear regression as it allows for the comparison of coefficients on an equal footing. When features are on different scales, it's challenging to discern which ones have a more significant impact on the dependent variable. By standardizing, each coefficient reflects the impact of a one standard deviation change in the feature, making it easier to understand and compare the relative importance of each feature in the model. Additionally, standardizing can also aid in the convergence of the algorithm, especially in optimization processes like gradient descent, by ensuring that all features contribute equally to the model's learning process.\n",
    "\n",
    "To calculate the z-score in python we can import a function from the scipy (scientific python) library. All you need to to is to take `non_binary` and pass `zscore` to the apply funciton and save it do a variable named `scaled`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b675029",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import zscore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c4a89a2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "b23ccfd4",
   "metadata": {},
   "source": [
    "Uncomment and runt the code below to check if you succeded with calculate. If successful, the average should be zero and the standard deviation should be 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11c09864",
   "metadata": {},
   "outputs": [],
   "source": [
    "# scaled.agg([\"std\", \"mean\"]).round()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f37d15d",
   "metadata": {},
   "source": [
    "## Train Test Split\n",
    "\n",
    "The train-test split is a fundamental step in machine learning where the dataset is divided into two parts: the training set and the testing set. The training set is used to train the machine learning model, while the testing set is used to evaluate its performance. This split is crucial as it helps in assessing how well the model generalizes to new, unseen data. Without this step, there's a risk of overfitting, where the model performs well on the training data but poorly on new data. The train-test split thus ensures a more realistic evaluation of the model's effectiveness and robustness.\n",
    "\n",
    "Set the `test_size=0.2`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78e3461c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c43591b",
   "metadata": {},
   "source": [
    "## RFE | Preprocessing Step to Find the Best Estimators/Features\n",
    "\n",
    "<strong> This step is a cool add-on that you don't need to over analyse. Just read the text and try to understand the code that has been given to you.</strong>\n",
    "\n",
    "Recursive feature elimination (RFE) is a feature selection technique used in machine learning and statistics to select the most important features (columns) for a predictive model. RFE works by recursively removing features from the model and building a new model with the remaining features, until a specified number of features are left. It is a wrapper method, meaning that it uses a specific model (e.g. linear regression, support vector machines, etc.) to determine which features to eliminate.\n",
    "\n",
    "The general steps for performing RFE are as follows:\n",
    "\n",
    "**1. Choose a model: First, you need to choose a model that you want to use to perform feature selection. Common models used for RFE include linear regression, logistic regression, and support vector machines.**\n",
    "\n",
    "\n",
    "**2. Specify the number of features: Next, you need to specify the number of features you want to end up with at the end of the feature selection process. This can be a fixed number or a percentage of the original features.**\n",
    "\n",
    "\n",
    "**3. Eliminate the least important feature: The model is trained on the original set of features, and the importance of each feature is determined by examining the model's coefficients or weights. The least important feature is then eliminated from the set of features.**\n",
    " \n",
    " \n",
    "**4. Build a new model: A new model is built using the remaining features, and steps 3 and 4 are repeated until the desired number of features is reached.**\n",
    "\n",
    "\n",
    "**5. Evaluate the model: Finally, the model is evaluated using a validation set or cross-validation to assess the performance of the selected features.**\n",
    "\n",
    "RFE can be a powerful feature selection technique, as it can help to improve the performance and interpretability of a model by selecting the most important features. However, it can be computationally intensive, especially when dealing with large numbers of features or complex models. It is also important to keep in mind that feature selection is not a panacea, and that it is always possible to select a suboptimal set of features. Therefore, it is always a good practice to combine feature selection with other techniques such as regularization and cross-validation to avoid overfitting and ensure robustness of the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f9f8c58",
   "metadata": {},
   "outputs": [],
   "source": [
    "#import libraries\n",
    "from sklearn.feature_selection import RFE\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "# Create a scikit-learn estimator from the OLS model\n",
    "estimator = LinearRegression()\n",
    "\n",
    "# Create the RFE object and fit the model\n",
    "rfe = RFE(estimator, n_features_to_select=13)\n",
    "rfe = rfe.fit(X_train, y_train)\n",
    "\n",
    "# Print the selected features\n",
    "print('Selected features:', X_train.columns[rfe.support_])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8a70264",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run this to move on with the columns selected after elimination.\n",
    "rfe_cols = X_train.columns[rfe.support_]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4f75279",
   "metadata": {},
   "source": [
    "## Build and Train predictive model\n",
    "\n",
    "Initiate you `LinearRegression()` and then fit it to your data but remember to fit it using the `rfe_cols`. After fitting the model, predict prices and save it to `y_pred`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adf7b61b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "2e897859",
   "metadata": {},
   "source": [
    "## Evaluate model on training data\n",
    "\n",
    "Have a look at mean squared error, r2 score, mean absolute percentage error, mean absolute error and write a few comments on how you interpret the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9620af6d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "e961df3e",
   "metadata": {},
   "source": [
    "## Evaluate model on test data\n",
    "\n",
    "Create a new `y_pred` for the test data. Then, have a look at mean squared error, r2 score, mean absolute percentage error, mean absolute error and write a few comments on how you interpret the results. How does the scores differ from the training data. Does your model generalize well or poorly?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db1f30f1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "7cb6c7f9",
   "metadata": {},
   "source": [
    "# Good Job!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
